# Self-Supervised-using-Knowledge-Distillation-
This method proposes a lightweight, efficient strategy for self-supervised representation learning on small-scale datasets, leveraging knowledge distillation (KD) to transfer generalizable knowledge from a powerful teacher model to a compact student networ
